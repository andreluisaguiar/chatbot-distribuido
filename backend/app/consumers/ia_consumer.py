# backend/app/consumers/ia_consumer.py

import pika # type: ignore
import os
import json
import time
import requests
import asyncio
import sys
from pathlib import Path
from http.server import HTTPServer, BaseHTTPRequestHandler # type: ignore
from threading import Thread # type: ignore
from prometheus_client import Counter, generate_latest, CONTENT_TYPE_LATEST # type: ignore
from dotenv import load_dotenv

# Tenta importar a biblioteca oficial do Google Gemini
try:
    from google import genai
    HAS_GOOGLE_GENAI = True
    print(" [INFO] Biblioteca 'google-genai' importada com sucesso!")
except ImportError as e:
    HAS_GOOGLE_GENAI = False
    print(f" [INFO] Biblioteca 'google-genai' n√£o encontrada. Usando m√©todo HTTP direto. Erro: {e}")

# For√ßa o flush imediato de stdout/stderr para Docker
sys.stdout.reconfigure(line_buffering=True)
sys.stderr.reconfigure(line_buffering=True)

# Carrega vari√°veis de ambiente do arquivo .env
load_dotenv()

# Adiciona o diret√≥rio raiz ao path para imports absolutos
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from app.services.rabbitmq_service import get_rabbitmq_connection
from app.services.database_service import save_message, AsyncSessionLocal # Para persist√™ncia real

# --- Configura√ß√µes (Lidas do .env) ---
RABBITMQ_HOST = os.getenv("RABBITMQ_HOST", "rabbitmq")
RABBITMQ_USER = os.getenv("RABBITMQ_USER", "user")
RABBITMQ_PASS = os.getenv("RABBITMQ_PASS", "password")
QUEUE_NAME = 'q.ia_request'
EXCHANGE_NAME = 'x.chat_requests'

RESPONSE_QUEUE_NAME = 'q.ia_response'
RESPONSE_EXCHANGE_NAME = 'x.chat_responses'

# Configura√ß√µes da API de IA
AI_API_KEY = os.getenv("AI_API_KEY")
AI_MODEL = os.getenv("AI_MODEL", "gpt-3.5-turbo")
AI_API_URL = os.getenv("AI_API_URL") 

# Detecta se √© API Gemini baseado no modelo
IS_GEMINI = AI_MODEL.startswith("gemini") if AI_MODEL else False
# Detecta se √© API DeepSeek baseado no modelo
IS_DEEPSEEK = AI_MODEL.startswith("deepseek") if AI_MODEL else False
# Detecta se √© API Groq baseado no modelo (llama, mixtral, etc)
IS_GROQ = AI_MODEL.startswith("llama") or AI_MODEL.startswith("mixtral") or AI_MODEL.startswith("gemma") if AI_MODEL else False

# Prompt de sistema focado em apoio a estudos
SYSTEM_PROMPT = """
Voc√™ √© o B.A.S.E. (Bot de Apoio a Sistemas e Estudos), um assistente virtual especializado na disciplina de Sistemas Distribu√≠dos.
Seu nome √© um acr√¥nimo para "Basically Available, Soft state, Eventual consistency".

Suas diretrizes de comportamento:
1. IDENTIDADE: Voc√™ N√ÉO deve iniciar toda frase se apresentando. Apresente-se sempre na primeira mensgaem e se o usu√°rio perguntar explicitamente "Quem √© voc√™?", "Qual seu nome?" ou na primeira mensagem da conversa.
2. FLUXO: Para perguntas de conte√∫do (ex: "O que √© RPC?"), v√° direto para a explica√ß√£o t√©cnica, sem sauda√ß√µes longas.
3. REFER√äNCIAS: Sempre que poss√≠vel, cite conceitos dos livros do Tanenbaum ou Coulouris.
4. DID√ÅTICA: Explique termos complexos com analogias do mundo real.
5. OBJETIVO: Ajude o aluno a entender o "porqu√™" das coisas.
"""

# M√©tricas de Throughput do Worker
messages_processed_total = Counter(
    'ia_worker_messages_processed_total',
    'Total de mensagens processadas pelo IA Worker',
    ['status']
)


class MetricsHandler(BaseHTTPRequestHandler):
    """Handler HTTP para expor m√©tricas Prometheus"""
    
    def do_GET(self):
        if self.path == '/metrics':
            self.send_response(200)
            self.send_header('Content-Type', CONTENT_TYPE_LATEST)
            self.end_headers()
            self.wfile.write(generate_latest())
        else:
            self.send_response(404)
            self.end_headers()
    
    def log_message(self, format, *args):
        # Suprime logs do servidor HTTP
        pass


def start_metrics_server(port=8000):
    """Inicia servidor HTTP para expor m√©tricas Prometheus"""
    try:
        server = HTTPServer(('0.0.0.0', port), MetricsHandler)
        print(f" [METRICS] Servidor de m√©tricas iniciado na porta {port}")
        server.serve_forever()
    except OSError as e:
        print(f" [METRICS ERROR] N√£o foi poss√≠vel iniciar servidor na porta {port}: {e}")
        # Tenta porta alternativa
        alt_port = 8001
        try:
            server = HTTPServer(('0.0.0.0', alt_port), MetricsHandler)
            print(f" [METRICS] Servidor de m√©tricas iniciado na porta alternativa {alt_port}")
            server.serve_forever()
        except Exception as e2:
            print(f" [METRICS ERROR] Falha ao iniciar servidor de m√©tricas: {e2}")

# Chamada real √† API Externa de IA
def call_external_ai_api(user_prompt: str):
    """
    Chama a API de IA (OpenAI ou compat√≠vel) para gerar resposta focada em estudos.
    
    Args:
        user_prompt: Mensagem do usu√°rio
        
    Returns:
        Resposta gerada pela IA ou mensagem de erro
    """
    if not AI_API_KEY:
        error_msg = "Erro: AI_API_KEY n√£o configurada. Configure a vari√°vel de ambiente AI_API_KEY."
        print(f" [!!!] {error_msg}")
        return error_msg
    
    api_type = "Groq" if IS_GROQ else ("DeepSeek" if IS_DEEPSEEK else ("Gemini" if IS_GEMINI else "OpenAI"))
    print(f" [+] [WORKER] Processando IA ({api_type}) para: '{user_prompt[:50]}...'")
    print(f" [+] [WORKER] Modelo: {AI_MODEL}, IS_GEMINI: {IS_GEMINI}, IS_DEEPSEEK: {IS_DEEPSEEK}, IS_GROQ: {IS_GROQ}, API_KEY presente: {bool(AI_API_KEY)}")
    
    try:
        # Determina URL e formato baseado no tipo de API
        if IS_GEMINI:
            # Tenta usar a biblioteca oficial do Google Gemini se dispon√≠vel
            if HAS_GOOGLE_GENAI:
                model_name = AI_MODEL if AI_MODEL else "gemini-2.0-flash"
                
                # Verifica se o modelo est√° dispon√≠vel no plano gratuito
                if model_name in ["gemini-3-pro-preview", "gemini-3-pro"]:
                    print(f" [!!!] AVISO: Modelo '{model_name}' n√£o est√° dispon√≠vel no plano gratuito. Usando 'gemini-2.0-flash'.")
                    model_name = "gemini-2.0-flash"
                
                max_retries_lib = 3
                retry_delay_lib = 2
                
                for attempt_lib in range(max_retries_lib):
                    try:
                        # Cria o cliente
                        client = genai.Client(api_key=AI_API_KEY)
                        
                        # Combina system prompt com user prompt
                        full_prompt = f"{SYSTEM_PROMPT}\n\nUsu√°rio: {user_prompt}\n\nAssistente:"
                        
                        print(f" [+] [WORKER] Usando biblioteca oficial Google Gemini (modelo: {model_name}, tentativa {attempt_lib + 1}/{max_retries_lib})")
                        
                        # Gera conte√∫do usando a biblioteca oficial
                        response = client.models.generate_content(
                            model=model_name,
                            contents=full_prompt
                        )
                        
                        bot_response = response.text
                        if bot_response:
                            print(f" [+] [WORKER] Resposta da IA (Gemini) gerada com sucesso ({len(bot_response)} caracteres)")
                            return bot_response
                        else:
                            return "Erro: A API retornou uma resposta vazia."
                            
                    except Exception as lib_error:
                        error_str = str(lib_error)
                        print(f" [!!!] Erro ao usar biblioteca oficial (tentativa {attempt_lib + 1}/{max_retries_lib}): {error_str}")
                        
                        # Se for rate limit (429) ou quota esgotada, verifica se √© quota 0
                        if "429" in error_str or "rate limit" in error_str.lower() or "quota" in error_str.lower() or "RESOURCE_EXHAUSTED" in error_str:
                            # Verifica se √© quota 0 (limit: 0) - significa que n√£o tem acesso ao plano gratuito
                            if "limit: 0" in error_str or '"limit": 0' in error_str:
                                # Se for quota 0, retorna mensagem clara sobre o problema
                                error_msg = (
                                    "‚ùå Erro: Sua API key do Google Gemini n√£o tem acesso ao plano gratuito ou a quota est√° zerada.\n\n"
                                    "üîç O que fazer:\n"
                                    "1. Acesse https://aistudio.google.com/\n"
                                    "2. Verifique se sua API key est√° ativa\n"
                                    "3. Verifique o uso e quotas em: https://ai.dev/usage?tab=rate-limit\n"
                                    "4. Certifique-se de que o projeto tem acesso ao plano gratuito habilitado\n"
                                    "5. Se necess√°rio, gere uma nova API key em um projeto diferente\n\n"
                                    "üí° Nota: Alguns modelos podem ter quota 0 no plano gratuito. Tente usar uma API key de um projeto que tenha acesso ao plano gratuito habilitado."
                                )
                                print(f" [!!!] {error_msg}")
                                return error_msg
                            
                            # Se for rate limit normal (n√£o quota 0), tenta novamente
                            if attempt_lib < max_retries_lib - 1:
                                wait_time = retry_delay_lib * (2 ** attempt_lib)
                                print(f" [!!!] Rate limit detectado. Aguardando {wait_time}s antes de tentar novamente...")
                                time.sleep(wait_time)
                                continue
                            else:
                                return "Erro: Rate limit da API de IA (muitas requisi√ß√µes). Por favor, aguarde alguns instantes e tente novamente."
                        
                        # Se for erro 404 (modelo n√£o encontrado), retorna mensagem espec√≠fica
                        if "404" in error_str or "NOT_FOUND" in error_str or "is not found" in error_str.lower():
                            error_msg = (
                                f"‚ùå Erro: Modelo '{model_name}' n√£o encontrado na API v1beta.\n\n"
                                "üîç O que fazer:\n"
                                "1. Verifique se o nome do modelo est√° correto\n"
                                "2. Tente usar um modelo dispon√≠vel no plano gratuito\n"
                                "3. Consulte a documenta√ß√£o: https://ai.google.dev/gemini-api/docs/models\n"
                                f"4. Modelo atual configurado: {AI_MODEL}"
                            )
                            print(f" [!!!] {error_msg}")
                            return error_msg
                        
                        # Se n√£o for rate limit e for a √∫ltima tentativa, faz fallback para HTTP
                        if attempt_lib == max_retries_lib - 1:
                            print(f" [!!!] Todas as tentativas com biblioteca oficial falharam. Tentando m√©todo HTTP direto como fallback...")
                            break
                        else:
                            # Para outros erros, tenta novamente
                            time.sleep(retry_delay_lib)
                            continue
            
            # M√©todo HTTP direto (fallback ou quando biblioteca n√£o est√° dispon√≠vel)
            # API do Google Gemini via HTTP
            # URL: https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={API_KEY}
            # Modelos v√°lidos no plano gratuito: gemini-2.0-flash, gemini-1.5-flash
            # NOTA: gemini-3-pro-preview N√ÉO est√° dispon√≠vel no plano gratuito (quota: 0)
            model_name = AI_MODEL if AI_MODEL else "gemini-2.0-flash"
            
            # Lista de modelos inv√°lidos ou n√£o dispon√≠veis no plano gratuito - substitui por modelo v√°lido
            # gemini-3-pro-preview n√£o est√° dispon√≠vel no plano gratuito (quota: 0)
            invalid_models = ["gemini-3", "gemini-pro", "gemini-pro-1.0", "gemini-3-pro-preview", "gemini-3-pro"]
            if model_name in invalid_models:
                print(f" [!!!] AVISO: Modelo '{model_name}' n√£o est√° dispon√≠vel no plano gratuito. Usando 'gemini-2.0-flash' como alternativa.")
                model_name = "gemini-2.0-flash"
            
            api_url = AI_API_URL or f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent"
            print(f" [+] [WORKER] URL da API Gemini: {api_url.split('?')[0]} (modelo: {model_name})")
            
            # Combina system prompt com user prompt para Gemini
            full_prompt = f"{SYSTEM_PROMPT}\n\nUsu√°rio: {user_prompt}\n\nAssistente:"
            
            headers = {
                "Content-Type": "application/json"
            }
            
            payload = {
                "contents": [{
                    "parts": [{
                        "text": full_prompt
                    }]
                }],
                "generationConfig": {
                    "temperature": 0.7,
                    "maxOutputTokens": 1000
                }
            }
            
            # Adiciona a chave como par√¢metro na URL para Gemini
            if "?" in api_url:
                api_url = f"{api_url}&key={AI_API_KEY}"
            else:
                api_url = f"{api_url}?key={AI_API_KEY}"
                
        elif IS_DEEPSEEK:
            # API DeepSeek (compat√≠vel com OpenAI)
            api_url = AI_API_URL or "https://api.deepseek.com/v1/chat/completions"
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {AI_API_KEY}"
            }
            
            payload = {
                "model": AI_MODEL,
                "messages": [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 1000
            }
        elif IS_GROQ:
            # API Groq (compat√≠vel com OpenAI)
            api_url = AI_API_URL or "https://api.groq.com/openai/v1/chat/completions"
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {AI_API_KEY}"
            }
            
            payload = {
                "model": AI_MODEL,
                "messages": [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 1000
            }
        else:
            # API OpenAI (padr√£o)
            api_url = AI_API_URL or "https://api.openai.com/v1/chat/completions"
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {AI_API_KEY}"
            }
            
            payload = {
                "model": AI_MODEL,
                "messages": [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 1000
            }
        
        # Faz a requisi√ß√£o HTTP com retry para rate limiting (429)
        print(f" [+] [WORKER] Enviando requisi√ß√£o para API de IA...")
        
        max_retries = 3
        retry_delay = 2  # Come√ßa com 2 segundos
        
        for attempt in range(max_retries):
            response = requests.post(
                api_url,
                headers=headers,
                json=payload,
                timeout=30  # Timeout de 30 segundos
            )
            
            print(f" [+] [WORKER] Resposta recebida: Status {response.status_code} (tentativa {attempt + 1}/{max_retries})")
            
            # Se for 429 (rate limit), verifica se √© quota 0 ou rate limit normal
            if response.status_code == 429:
                try:
                    error_data = response.json()
                    error_text = json.dumps(error_data)
                    # Verifica se √© quota 0 (limit: 0)
                    if "limit: 0" in error_text or '"limit": 0' in error_text:
                        error_msg = (
                            "‚ùå Erro: Sua API key do Google Gemini n√£o tem acesso ao plano gratuito ou a quota est√° zerada.\n\n"
                            "üîç O que fazer:\n"
                            "1. Acesse https://aistudio.google.com/\n"
                            "2. Verifique se sua API key est√° ativa\n"
                            "3. Verifique o uso e quotas em: https://ai.dev/usage?tab=rate-limit\n"
                            "4. Certifique-se de que o projeto tem acesso ao plano gratuito habilitado\n"
                            "5. Se necess√°rio, gere uma nova API key em um projeto diferente\n\n"
                            "üí° Nota: Alguns modelos podem ter quota 0 no plano gratuito. Tente usar uma API key de um projeto que tenha acesso ao plano gratuito habilitado."
                        )
                        print(f" [!!!] {error_msg}")
                        return error_msg
                except:
                    pass
                
                # Se for rate limit normal (n√£o quota 0), tenta novamente
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)  # Backoff exponencial: 2s, 4s, 8s
                    print(f" [!!!] Rate limit atingido (429). Aguardando {wait_time}s antes de tentar novamente...")
                    time.sleep(wait_time)
                    continue
                else:
                    error_msg = "Erro: Rate limit da API de IA (muitas requisi√ß√µes). Por favor, aguarde alguns instantes e tente novamente."
                    print(f" [!!!] {error_msg}")
                    return error_msg
            
            # Se n√£o for 429, sai do loop de retry
            break
        
        # Verifica se a requisi√ß√£o foi bem-sucedida
        if response.status_code == 200:
            response_data = response.json()
            
            # Extrai a resposta baseado no tipo de API
            if IS_GEMINI:
                # Formato Gemini: {"candidates": [{"content": {"parts": [{"text": "..."}]}}]}
                if "candidates" in response_data and len(response_data["candidates"]) > 0:
                    candidate = response_data["candidates"][0]
                    if "content" in candidate and "parts" in candidate["content"]:
                        bot_response = candidate["content"]["parts"][0].get("text", "")
                        if bot_response:
                            print(f" [+] [WORKER] Resposta da IA (Gemini) gerada com sucesso ({len(bot_response)} caracteres)")
                            return bot_response
                
                error_msg = "Erro: Resposta da API Gemini n√£o cont√©m 'candidates' v√°lido."
                print(f" [!!!] {error_msg}")
                print(f" [!!!] Resposta da API: {response_data}")
                return error_msg
            else:
                # Formato OpenAI/DeepSeek/Groq: {"choices": [{"message": {"content": "..."}}]}
                if "choices" in response_data and len(response_data["choices"]) > 0:
                    bot_response = response_data["choices"][0]["message"]["content"]
                    api_name = "Groq" if IS_GROQ else ("DeepSeek" if IS_DEEPSEEK else "OpenAI")
                    print(f" [+] [WORKER] Resposta da IA ({api_name}) gerada com sucesso ({len(bot_response)} caracteres)")
                    return bot_response
                else:
                    error_msg = "Erro: Resposta da API n√£o cont√©m 'choices' v√°lido."
                    print(f" [!!!] {error_msg}")
                    print(f" [!!!] Resposta da API: {response_data}")
                    return error_msg
        else:
            error_msg = f"Erro na API de IA: Status {response.status_code} - {response.text[:500]}"
            print(f" [!!!] {error_msg}")
            print(f" [!!!] Resposta completa: {response.text}")
            
            # Tratamento espec√≠fico para diferentes c√≥digos de erro
            if response.status_code == 400:
                # Erro 400: Pode ser modelo descontinuado ou inv√°lido
                try:
                    error_data = response.json()
                    error_text = json.dumps(error_data)
                    if "decommissioned" in error_text.lower() or "no longer supported" in error_text.lower():
                        return (
                            f"‚ùå Erro: O modelo '{AI_MODEL}' foi descontinuado e n√£o √© mais suportado.\n\n"
                            "üîç O que fazer:\n"
                            "1. Verifique os modelos dispon√≠veis em: https://console.groq.com/docs/models\n"
                            "2. Atualize o modelo no arquivo .env\n"
                            "3. Modelos sugeridos: llama-3.3-70b-versatile, llama-3.3-8b-instant, mixtral-8x7b-32768\n\n"
                            f"üí° Modelo atual configurado: {AI_MODEL}"
                        )
                except:
                    pass
            elif response.status_code == 402:
                # Erro 402: Saldo insuficiente (DeepSeek)
                try:
                    error_data = response.json()
                    if "Insufficient Balance" in str(error_data) or "insufficient" in str(error_data).lower():
                        return (
                            "‚ùå Erro: Saldo insuficiente na conta da API DeepSeek.\n\n"
                            "üîç O que fazer:\n"
                            "1. Acesse https://platform.deepseek.com/\n"
                            "2. Verifique o saldo da sua conta\n"
                            "3. Adicione cr√©ditos se necess√°rio\n"
                            "4. Verifique se a API key est√° correta e ativa\n\n"
                            "üí° Nota: A API DeepSeek requer cr√©ditos na conta para funcionar."
                        )
                except:
                    pass
                return "Erro: Saldo insuficiente na conta da API. Por favor, adicione cr√©ditos √† sua conta."
            elif response.status_code == 404 and IS_GEMINI:
                return f"Erro: O modelo '{AI_MODEL}' n√£o foi encontrado. Por favor, verifique se o modelo est√° correto (ex: gemini-2.0-flash, gemini-1.5-flash)."
            elif response.status_code == 429:
                return f"Erro: Rate limit da API de IA (muitas requisi√ß√µes). Por favor, aguarde alguns instantes e tente novamente."
            elif response.status_code == 401:
                return f"Erro: Chave de API inv√°lida ou expirada. Por favor, verifique a configura√ß√£o da API."
            elif response.status_code == 403:
                return f"Erro: Acesso negado √† API. Verifique as permiss√µes da sua chave de API."
            else:
                return f"Desculpe, ocorreu um erro ao processar sua mensagem (Status {response.status_code}). Por favor, tente novamente."
            
    except requests.exceptions.Timeout:
        error_msg = "Erro: Timeout ao chamar API de IA (mais de 30 segundos)."
        print(f" [!!!] {error_msg}")
        return "Desculpe, a resposta est√° demorando mais que o esperado. Por favor, tente novamente."
    except requests.exceptions.RequestException as e:
        error_msg = f"Erro de conex√£o com API de IA: {str(e)}"
        print(f" [!!!] {error_msg}")
        return "Desculpe, n√£o foi poss√≠vel conectar ao servi√ßo de IA. Por favor, tente novamente mais tarde."
    except Exception as e:
        error_msg = f"Erro inesperado ao chamar API de IA: {str(e)}"
        print(f" [!!!] {error_msg}")
        import traceback
        traceback.print_exc()
        return "Desculpe, ocorreu um erro inesperado. Por favor, tente novamente."

# --- L√≥gica de Publica√ß√£o de Resposta ---
def publish_response(user_id: str, bot_response: str):

    try:
        connection = get_rabbitmq_connection()
        channel = connection.channel()

        # Garante que a Exchange e a Fila de RESPOSTA existam (Resili√™ncia/OS5)
        channel.exchange_declare(exchange=RESPONSE_EXCHANGE_NAME, exchange_type='direct', durable=True)
        channel.queue_declare(queue=RESPONSE_QUEUE_NAME, durable=True)
        channel.queue_bind(exchange=RESPONSE_EXCHANGE_NAME, queue=RESPONSE_QUEUE_NAME, routing_key=RESPONSE_QUEUE_NAME)

        response_payload = {
            "user_id": user_id,
            "bot_content": bot_response, 
            "timestamp_processed": time.time()
        }
        
        body = json.dumps(response_payload)
        
        channel.basic_publish(
            exchange=RESPONSE_EXCHANGE_NAME,
            routing_key=RESPONSE_QUEUE_NAME,
            body=body,
            properties=pika.BasicProperties(
                delivery_mode=pika.spec.PERSISTENT_DELIVERY_MODE
            )
        )
        
        print(f" [->] Resposta enviada para fila: {RESPONSE_QUEUE_NAME}")
        connection.close()
        return True

    except Exception as e:
        print(f" [!] ERRO ao publicar resposta na fila: {e}")
        return False

# --- L√≥gica de Callback e Consumo ---
def callback(ch, method, properties, body):

    try:
        message_data = json.loads(body)
        
        user_id = message_data.get("user_id")
        user_prompt = message_data.get("content")
        
        if not user_id or not user_prompt:
             print(" [!] Mensagem incompleta recebida. Ignorando.")
             ch.basic_ack(delivery_tag=method.delivery_tag)
             return

        # 1. Processamento da IA (Etapa Lenta)
        bot_response = call_external_ai_api(user_prompt)
        
        # 2. Persist√™ncia da Resposta do Bot
        print(f" [DB] Salvando resposta do BOT no PostgreSQL para usu√°rio {user_id}...")
        
        async def save_bot_message_async():
             async with AsyncSessionLocal() as db_session:
                await save_message(db_session, user_id, "BOT", bot_response)

        # Executa a fun√ß√£o ass√≠ncrona de forma s√≠ncrona
        asyncio.run(save_bot_message_async())
        
        # 3. Publicar a Resposta na Fila q.ia_response
        publish_response(user_id, bot_response)
        
        # 4. Registra m√©trica de throughput (mensagem processada com sucesso)
        messages_processed_total.labels(status='success').inc()
        
        # 5. Confirma√ß√£o (ACK)
        ch.basic_ack(delivery_tag=method.delivery_tag) 

    except Exception as e:
        print(f" [!!!] Erro no processamento do Worker: {e}. Rejeitando mensagem.")
        # Registra m√©trica de falha
        messages_processed_total.labels(status='error').inc()
        # Rejeita a mensagem para que ela volte √† fila ou v√° para uma DLQ
        ch.basic_nack(delivery_tag=method.delivery_tag) 


def start_consuming():
    """Conecta ao RabbitMQ e inicia o loop de consumo da fila de requisi√ß√£o."""
    credentials = pika.PlainCredentials(RABBITMQ_USER, RABBITMQ_PASS)
    parameters = pika.ConnectionParameters(
        host=RABBITMQ_HOST,
        credentials=credentials
    )

    try:
        connection = pika.BlockingConnection(parameters)
        channel = connection.channel()

        # Garante que a fila e a exchange existam (durable=True para Resili√™ncia)
        channel.exchange_declare(exchange=EXCHANGE_NAME, exchange_type='direct', durable=True)
        channel.queue_declare(queue=QUEUE_NAME, durable=True)
        channel.queue_bind(exchange=EXCHANGE_NAME, queue=QUEUE_NAME, routing_key=QUEUE_NAME)
        
        # Fair dispatch (Qualidade de Servi√ßo - QoS)
        channel.basic_qos(prefetch_count=1)

        print(f' [*] Worker IA iniciado. Aguardando mensagens na fila {QUEUE_NAME}.')
        
        channel.basic_consume(queue=QUEUE_NAME, on_message_callback=callback)
        channel.start_consuming()

    except pika.exceptions.AMQPConnectionError as e:
        print(f" [!!!] Erro de conex√£o com RabbitMQ. Tentando reconectar em 5s: {e}")
        time.sleep(5)
        start_consuming() # Tenta reconectar (Resili√™ncia)
    except KeyboardInterrupt:
        print('Worker desligado.')

if __name__ == '__main__':
    # Log de inicializa√ß√£o com informa√ß√µes de configura√ß√£o
    print("=" * 60)
    print(" [WORKER] Iniciando IA Worker...")
    print(f" [WORKER] Modelo configurado: {AI_MODEL}")
    api_type_startup = "Groq" if IS_GROQ else ("DeepSeek" if IS_DEEPSEEK else ("Gemini" if IS_GEMINI else "OpenAI"))
    print(f" [WORKER] Tipo de API: {api_type_startup}")
    print(f" [WORKER] API Key presente: {'Sim' if AI_API_KEY else 'N√ÉO - ERRO!'}")
    print(f" [WORKER] URL da API: {AI_API_URL or 'Usando padr√£o'}")
    print("=" * 60)
    
    if not AI_API_KEY:
        print(" [!!!] ERRO CR√çTICO: AI_API_KEY n√£o configurada!")
        print(" [!!!] Configure a vari√°vel AI_API_KEY no arquivo .env")
        exit(1)
    
    # Inicia servidor de m√©tricas em thread separada
    metrics_port = int(os.getenv("METRICS_PORT", "8000"))
    metrics_thread = Thread(target=start_metrics_server, args=(metrics_port,), daemon=True)
    metrics_thread.start()
    
    # Inicia consumo de mensagens
    start_consuming()